from django.conf import settings

# Combine Install Location
COMBINE_INSTALL_PATH = '/opt/combine'


# Combine Front-End
APP_HOST = '192.168.45.10'


# Spark / YARN tuning
SPARK_MAX_WORKERS = 1
JDBC_NUMPARTITIONS = 10
SPARK_REPARTITION = (SPARK_MAX_WORKERS * 8)


# Apache Livy settings
'''
Combine uses Livy to issue spark statements.
Livy provides a stateless pattern for interacting with Spark, and by proxy, DPLA code.
'''
LIVY_HOST = 'localhost'
LIVY_PORT = 8998
LIVY_DEFAULT_SESSION_CONFIG = {
    'kind':'pyspark',
    'jars':[],
    'files':[
    	'file://%s/core/spark/es.py' % COMBINE_INSTALL_PATH.rstrip('/'),
    	'file://%s/core/spark/jobs.py' % COMBINE_INSTALL_PATH.rstrip('/'),
    	'file://%s/core/spark/record_validation.py' % COMBINE_INSTALL_PATH.rstrip('/'),
    	'file://%s/core/spark/utils.py' % COMBINE_INSTALL_PATH.rstrip('/'),
    ]
}


# Storage for avro files and other binary files
'''
Make sure to note file:// or hdfs:// prefix
'''
BINARY_STORAGE = 'file:///home/combine/data/combine'
WRITE_AVRO = False


# ElasicSearch server
ES_HOST = '192.168.45.10'
INDEX_TO_ES = True


# ElasticSearch indexing
INCLUDE_ATTRIBUTES_GENERIC_MAPPER = True


# ElasticSearch analysis
CARDINALITY_PRECISION_THRESHOLD = 100
ONE_PER_DOC_OFFSET = 0.05


# Service Hub
SERVICE_HUB_PREFIX = 'mi--'


# OAI Server
OAI_RESPONSE_SIZE = 500
COMBINE_OAI_IDENTIFIER = 'oai:michigan'
METADATA_PREFIXES = {
	'mods':{
			'schema':'http://www.loc.gov/standards/mods/v3/mods.xsd',
			'namespace':'http://www.loc.gov/mods/v3'
		},
	'oai_dc':{
			'schema':'http://www.openarchives.org/OAI/2.0/oai_dc.xsd',
			'namespace':'http://purl.org/dc/elements/1.1/'
		},
	'dc':{
			'schema':'http://www.openarchives.org/OAI/2.0/oai_dc.xsd',
			'namespace':'http://purl.org/dc/elements/1.1/'
		},
}


# Database configurations for use in Spark context
COMBINE_DATABASE = {
	'jdbc_url':'jdbc:mysql://localhost:3306/combine',
	'user':settings.DATABASES['default']['USER'],
	'password':settings.DATABASES['default']['PASSWORD']
}

# DPLA API
DPLA_API_KEY = None


# AWS S3 Credentials
AWS_ACCESS_KEY_ID = None
AWS_SECRET_ACCESS_KEY = None
DPLA_S3_BUCKET = 'dpla-provider-export'


# Analysis Jobs Org and Record Group
'''
This dictionary provides the name of the Organization and Record Group that Analysis Jobs will be created under.
Because Analysis jobs are extremely similar to other workflow jobs, but do not lend themselves towards the established
Organization --> Record Group --> Job hierarchy, this ensures they are treated similarily to other jobs, but skip the
need for users to manually create these somewhat unique Organization and Record Group.  
	- it is recommended to make these names quite unique, to avoid clashing with user created Orgs and Record Groups
	- the Organization and Record Group names defined in ANALYSIS_JOBS_HIERARCHY will NOT show up in any Org or Record
	Group views or other workflows
	- it is quite normal, and perhaps even encouraged, to leave these as the defaults provided
'''
ANALYSIS_JOBS_HIERARCHY = {
	'organization':'AnalysisOrganizationf8ed4bfcefc4dbf87b588a5de9b7cc95', # suffix is md5 hash of 'AnalysisOrganization'
	'record_group':'AnalysisRecordGroupf660bb4826bea8b63fd773d27d687cfd' # suffix is md5 hash of 'AnalysisRecordGroup'
}


# Django-Background-Tasks
# http://django-background-tasks.readthedocs.io/en/latest/#
MAX_ATTEMPTS = 3
MAX_RUN_TIME = 14800
BACKGROUND_TASK_RUN_ASYNC = False
